
# How to reduce variance?  -> Regularization

so we introduce another parameter lambda which penalizes the value of W to reduce overfitting,   
it can be thought of like this -   
z = w*a +b   
so we increase lambda  which inturn decreases w which decreases z and that shifts  it from exponential to somewhat linear for all layers.   
which is ultimatly reducing overfitting.



# DropOut - another regularization technique

randomly zeroing out some hidden units to avoid overfitting.


# Data Augmentation

Making more training data by manipulating the image such as rotating , flipping , tilting , removing R or G or B from the image.
