Neural networks
Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns. They interpret sensory data through a kind of machine perception, labeling or clustering raw input. The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text or time series, must be translated.

Linear regression
linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables).

Logistic regression
the logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass/fail, win/lose. uses logistic function to model binary dependent model.

RNNs
Special type of NN. RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being depended on the previous computations. Another way to think about RNNs is that they have a “memory” which captures information about what has been calculated so far.

GRU - gated recurrant units
A modified form of RNNs.
Gated recurrent units help to adjust neural network input weights to solve the vanishing gradient problem that is a common issue with recurrent neural networks.
Gated recurrent units have what's called an update gate and a reset gate. Using these two vectors, the model refines outputs by 
controlling the flow of information through the model. 

LSTMS: long short term memory
A improvement over RNNs which solves vanishing and exploding gradient problem.

Vanishing problem:
As more layers using certain activation functions are added to neural networks, the gradients of the loss function approaches zero, making the network hard to train.
Certain activation functions, like the sigmoid function, squishes a large input space into a small input space between 0 and 1. Therefore, a large change in the input of the sigmoid function will cause a small change in the output. Hence, the derivative becomes small.

Exploding gradient proeblem
if the value of gradients are >1 it so happens that it increases exponentially due to many layers and approach a NaN value which leads to stop in training of NN.

NMT : netural machine translation
Its a technique in ML which takes two langauge and establishes a connection to convert one to other.

Encoder-Decoder Model : 
Used to do NMT
consists of two RNNs where one of them(encoder) captures the meaning of a sentence by concaternating sentence in it and vector which is fed in the second RNN(decoder) which extracts meaning from the vector using at each time step and uses the input from previous time step to compute next predicted value. 

Seq2Seq Model : 
another name for Encoder-decoder model

Attention Model:
Improvement over seq2seq model by taking attention from each input at each time step and using it along with previous time step output to predict next word.
It solves the long sentence forgetting problem of Seq2Seq model.


Deep learning :
Fancy name for ML


Project:
QUESTION GENERATION FROM ARTICLE 
1.Took a sentence 
2. extracted subject , object and numbers from it using stanford dependency parser 
3. passed sub,obj and num to a model which generates quesition given a ans 
4. print the generated questions

FOLLOW-UP QUESTION GENERATION:--------------------------------


QUESTION ANSWERING MODEL FROM WIKIPEDIA TRAINED ON SQUAD DATASET : ----------------------------------------
