{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is implementation of emojify of sequence models week 2 assignment from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from emo_utils import * #present in the folder \n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "numberOfEmojiUsed = 5\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function emoji_lis at 0x7ff20009f2f0>\n"
     ]
    }
   ],
   "source": [
    "print(emoji.emoji_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove.6B.50d.txt - download this file and place in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>French macaroon is so tasty</th>\n",
       "      <th>4</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>work is horrible</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am upset</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>throw the ball</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good joke</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is your favorite baseball game</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           French macaroon is so tasty  4  Unnamed: 2 Unnamed: 3\n",
       "0                     work is horrible  3         NaN        NaN\n",
       "1                           I am upset  3         NaN        [3]\n",
       "2                       throw the ball  1         NaN        [2]\n",
       "3                            Good joke  2         NaN        NaN\n",
       "4  what is your favorite baseball game  1         NaN        NaN"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filetest = pd.read_csv('data/emojify_data.csv')\n",
    "filetest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv( filename = 'data/emojify_data.csv'):                 #read emoji data and return phrases and emojis\n",
    "    #in pyhton the initialized data is taken if nothing is passed\n",
    "    phrases = [] #list containing all the sentences\n",
    "    emoji = []\n",
    "    file = open(filename,'r')\n",
    "    csvReader = csv.reader(file)\n",
    "    print(csvReader)\n",
    "    for rows in csvReader: \n",
    "        phrases.append(rows[0]) #row[0] == column[0][i] see  above\n",
    "        emoji.append(rows[1]) \n",
    "    file.close()\n",
    "    X = np.asarray(phrases) \n",
    "    #Input data, in any form that can be converted to an array. This includes lists,\n",
    "    #lists of tuples, tuples, tuples of tuples, tuples of lists and ndarrays.\n",
    "    Y = np.asarray(emoji,dtype=(int))\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_csv.reader object at 0x7ff1e0cefac8>\n",
      "['French macaroon is so tasty' 'work is horrible' 'I am upset'\n",
      " 'throw the ball' 'Good joke' 'what is your favorite baseball game'\n",
      " 'I cooked meat' 'stop messing around' 'I want chinese food'\n",
      " 'Let us go play baseball']\n"
     ]
    }
   ],
   "source": [
    "#testing the above function\n",
    "xtemp , ytemp = read_csv()\n",
    "print(xtemp[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_csv.reader object at 0x7ff1e0cefb38>\n",
      "<_csv.reader object at 0x7ff1e0cefb38>\n"
     ]
    }
   ],
   "source": [
    "#loading small dataset with 5 lables only\n",
    "xTrain ,yTrain = read_csv('data/smallTrain5.csv')\n",
    "xTest ,yTest = read_csv('data/smallTest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    # :heart: prints a black instead of red heart depending on the font\n",
    "                    \"1\": \":baseball:\",\n",
    "                    \"2\": \":smile:\",\n",
    "                    \"3\": \":disappointed:\",\n",
    "                    \"4\": \":fork_and_knife:\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_emoji(label):\n",
    "    \"\"\"\n",
    "    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
    "    \"\"\"\n",
    "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('never talk to me again', 3, 'ðŸ˜ž')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain[0],yTrain[0],label_to_emoji(yTrain[0]) #lable_to_emoji convert lable to emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y\n",
    "#eye - Return a 2-D array with ones on the diagonal and zeros elsewhere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 3 0 4 0 3 2 3 1 3 3 1 3 2 3 2 3 1 2 3 0 2 2 2 1 4 2 2 4 0 3 4 2 0 3 2\n",
      " 2 3 4 2 2 0 2 3 0 3 2 4 3 0 3 3 3 4 2 1 1 1 2 3 1 0 0 0 3 4 4 2 2 1 2 0 3\n",
      " 2 2 0 0 3 1 2 1 2 2 4 3 3 2 4 0 0 0 3 3 3 2 0 1 2 3 0 2 2 2 3 2 2 2 4 1 1\n",
      " 3 3 4 1 2 1 1 3 1 0 4 0 3 3 4 4 1 4 3 0 2]\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "testing = yTrain\n",
    "print(testing)\n",
    "newarray = np.eye(5)\n",
    "print(newarray)\n",
    "newarray1 = np.eye(5)[testing.reshape(-1)]\n",
    "print(newarray1)\n",
    "#i got this error\n",
    "#arrays used as indices must be of integer (or boolean) type\n",
    "#why?\n",
    "#i didn't type case the emoji while returning in read_csv\n",
    "\n",
    "#so testing.reshape(-1) converts everything into one row\n",
    "#oh man this is one-hot encoding!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of the model is a string corresponding to a sentence (e.g. \"I love you). In the code, the output will be a probability vector of shape (1,5), that you then pass in an argmax layer to extract the index of the most likely emoji output.\n",
    "\n",
    "To get our labels into a format suitable for training a softmax classifier, lets convert $Y$ from its current shape current shape $(m, 1)$ into a \"one-hot representation\" $(m, 5)$, where each row is a one-hot vector giving the label of one example,  \n",
    "\n",
    "This is why we did one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOneHotTrain = convert_to_one_hot(yTrain,numberOfEmojiUsed)\n",
    "YOneHotTest = convert_to_one_hot(yTest,numberOfEmojiUsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a BaseLine Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* since we have to make a model and models do not work with text we need to convert text to numbers for this we will use word embeddings of 50 dimentions\n",
    "\n",
    "a line in word embeddings look like this\n",
    "\n",
    "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaron', 'bkash', 'kumar', 'saharsh']\n"
     ]
    }
   ],
   "source": [
    "testIfListCanBeSorted = ['saharsh', 'kumar' , 'aaron' , 'bkash']\n",
    "print(sorted(testIfListCanBeSorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs():\n",
    "    fileName = 'data/glove.6B.50d.txt'\n",
    "    file = open(fileName,'r')\n",
    "    words = []\n",
    "    word_to_vec_map = {}\n",
    "    for line in file:              \n",
    "        line = line.strip().split()      #line is like ['the' , '0.418' , ' 0.24968' , '-0.41242' , '0.1217'] \n",
    "        currWordFromGlove = line[0]\n",
    "        words.append(currWordFromGlove) \n",
    "        word_to_vec_map[currWordFromGlove] = np.array(line[1:],dtype=np.float64)\n",
    "        #why np.float64?\n",
    "        #They are the same number. What differs is their representation; the Python native type uses\n",
    "        #a \"sane\" representation, and the NumPy type uses an accurate representation\n",
    "    i = 1\n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "    for word in sorted(words):\n",
    "        word_to_index[word] = i\n",
    "        index_to_word[i] = word\n",
    "        i += 1\n",
    "    return word_to_index , index_to_word , word_to_vec_map\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `word_to_index`: dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "- `index_to_word`: dictionary mapping from indices to their corresponding words in the vocabulary\n",
    "- `word_to_vec_map`: dictionary mapping words to their GloVe vector representation.\n",
    "\n",
    "#### testing the word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337302\n",
      "claming\n"
     ]
    }
   ],
   "source": [
    "print(word_to_index[\"something\"])\n",
    "print(index_to_word[102013])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next is to convert a sentence to word embedding representation so we take avg of  all words in a sentence so we get a final 50 1d array of 50 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector(sentence,word_to_vec_map):\n",
    "    sentenceSplitted = [word.lower() for word in sentence.split()]\n",
    "    \n",
    "    avgReturn = np.zeros((50,),dtype=np.float64)\n",
    "    for word in sentenceSplitted:\n",
    "\n",
    "        avgReturn += word_to_vec_map[word]\n",
    "    avgReturn = avgReturn/len(sentenceSplitted)\n",
    "    return avgReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
      " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
      "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
      "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
      "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
      "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
      " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
      " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
      "  0.1445417   0.09808667]\n"
     ]
    }
   ],
   "source": [
    "#testing this\n",
    "s = \"Morrocan couscous is my favorite dish\"\n",
    "print(sentence_to_vector(s,word_to_vec_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting part MAKING A MODEL!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    Arguments:\n",
    "\n",
    "    X -- input data, numpy array of sentences as strings, of shape (m, 1)\n",
    "    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    learning_rate -- learning_rate for the stochastic gradient descent algorithm\n",
    "    num_iterations -- number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    pred -- vector of predictions, numpy-array of shape (m, 1)\n",
    "    W -- weight matrix of the softmax layer, of shape (n_y, n_h)\n",
    "    b -- bias of the softmax layer, of shape (n_y,)\n",
    "    \n",
    "    variables:\n",
    "    n_y = noOfEmojiUsed\n",
    "    n_h = dimensions of glove vectors\n",
    "    m = no of training examples\n",
    "    num_iterations = epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/1.png)\n",
    "\n",
    "we will be using these equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp = np.exp(z - np.max(z))\n",
    "    \n",
    "    return exp/np.sum(exp)\n",
    "#in coursera assignment a extra term np.max(z) is subtracted from the numerator which i can't understand why\n",
    "#we'll check accuracy using both of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,Y,W,b,word_to_vec_map):\n",
    "    #Given X (sentences) and Y (emoji indices), \n",
    "    #predict emojis and compute the accuracy of your model over the given set.\n",
    "    #pred -- numpy array of shape (m, 1) with your predictions\n",
    "\n",
    "    m = X.shape[0]\n",
    "    pred = np.zeros(Y.shape)\n",
    "    for j in range(m):\n",
    "        #for each sentence find its's vector representation\n",
    "        avg = sentence_to_vector(X[j],word_to_vec_map)\n",
    "        #somthing related to SVD and and fromula mentioned above\n",
    "        Z = np.dot(W,avg) + b\n",
    "        A = softmax(Z)\n",
    "        pred[j] = np.argmax(A)\n",
    "    print(\"Accuracy; \" + str(np.mean((pred[:] == Y.reshape(m,1)[:]))))\n",
    "    return pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X,Y,word_to_vec_map, noOfEmojiUsed, learning_rate = 0.01 , num_iterations = 400):\n",
    "    #we are making an NN from scratch\n",
    "    np.random.seed(1) #random number every time random fucntion is called\n",
    "    #X = array of strings\n",
    "    #Y = array of labels(int)\n",
    "    #initilization\n",
    "    m = Y.shape[0]\n",
    "    n_y = noOfEmojiUsed\n",
    "    n_h = 50\n",
    "    \n",
    "    W = np.random.randn(n_y,n_h)/np.sqrt(n_h) #here 2d array filled with random value and shape 5,50\n",
    "    b = np.zeros((n_y,)) #seems there is only one layer maybe!?\n",
    "    YOneHot = convert_to_one_hot(Y,n_y) #to feed in NN\n",
    "    #optimization of weights and biases\n",
    "    for t in range(num_iterations):                   ##loop over number of iterations \n",
    "        for i in range(m):                           ##loop over number of training examples\n",
    "            #converting sentence into vector\n",
    "            avg = sentence_to_vector(X[i],word_to_vec_map)\n",
    "            #using the formula and forward propagating\n",
    "            z = np.dot(W,avg) + b\n",
    "            a = softmax(z)\n",
    "            #last formula is simple term by term multiplication as y_oh = 5*m and a =50*1\n",
    "            cost = -np.sum(YOneHot*a)\n",
    "            \n",
    "            #now gradients \n",
    "            #these gradients are calculated using stochastic Gradient Decent\n",
    "            dz = a - YOneHot[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1) , avg.reshape(1,n_h)) #n_h is 50\n",
    "            db = dz\n",
    "            \n",
    "            #updating parameters , algorithm followed is SVD\n",
    "            W = W - learning_rate*dW\n",
    "            b = b - learning_rate*db\n",
    "            \n",
    "        if t%50 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \"---cost = \" + str(cost)  )\n",
    "            pred = predict(X,Y,W,b,word_to_vec_map)\n",
    "        return pred,W,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Epoch: 0---cost = -24.920932302238263\n",
      "Accuracy; 0.2486799816345271\n",
      "[3. 0. 2. 3. 2. 3. 3. 2. 2. 3. 3. 3. 3. 3. 3. 3. 4. 3. 3. 4. 3. 1. 3. 3.\n",
      " 4. 2. 3. 3. 3. 4. 3. 3. 3. 3. 2. 3. 3. 0. 3. 4. 3. 3. 3. 4. 2. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 1. 3. 3. 3. 1. 3. 3. 3. 3. 3. 4. 1. 2. 1. 0.\n",
      " 2. 3. 3. 3. 3. 3. 3. 1. 3. 0. 4. 0. 3. 3. 3. 2. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 2. 3. 3. 3. 2. 3. 1. 3. 3. 2. 3. 3. 3. 3. 2. 3. 3. 3. 4. 3. 2. 3. 4.\n",
      " 3. 4. 1. 3. 3. 4. 2. 3. 3. 3. 3. 1.]\n"
     ]
    }
   ],
   "source": [
    "pred, W, b = model(xTrain,yTrain,word_to_vec_map,numberOfEmojiUsed)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
