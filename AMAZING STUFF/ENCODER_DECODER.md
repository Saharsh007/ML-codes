## No of parameters in lstm - https://www.youtube.com/watch?v=l5TAISVlYM0&feature=youtu.be

## embedding layer in keras - https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work  

## question answering model using seq2seq model  - https://hackernoon.com/implementing-a-sequence-to-sequence-model-45a6133958cas

## very awesome projects - https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639

## good read for google tranlator -- https://smerity.com/articles/2016/google_nmt_arch.html

## using encoder decoder for machine translation - -https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/

## https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-2-seq2seq-model-encoderdecoder-model-6c22e29fd7e1


## text summarition using  attention models - https://machinelearningmastery.com/encoder-decoder-models-text-summarization-keras/

## The Unreasonable Effectiveness of Recurrent Neural Networks by andre karpathy - http://karpathy.github.io/2015/05/21/rnn-effectiveness/


## understand lstms - http://colah.github.io/posts/2015-08-Understanding-LSTMs/

## backprop = https://machinelearningmastery.com/gentle-introduction-backpropagation-time/

## a good chatbot - https://github.com/gunthercox/chatterBot