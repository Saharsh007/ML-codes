# Machine Translation using seq2seq model from English to French

## char level seq2seq model as we as word level seq2seq model is used  
## in char level we try to predict the next char and encode the data as characters , no issue of large matrix as types of characters are limited  
## whereas in word level at least 12gigs of memory is needed to encoded all the ~14k examples
## can be done on kaggle kernal or google colab
